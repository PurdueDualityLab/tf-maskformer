{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "from official.projects.detr.modeling.detr import position_embedding_sine\n",
    "from official.modeling import tf_utils\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 nhead, \n",
    "                 dropout=0.0, \n",
    "                 activation=\"relu\", \n",
    "                 normalize_before=False,\n",
    "                 **kwargs):\n",
    "        self._d_model = d_model\n",
    "        self._nhead = nhead\n",
    "        self._dropout = dropout\n",
    "        self._activation = activation\n",
    "        self._normalize_before = normalize_before\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        self.self_attn = tf.keras.layers.MultiHeadAttention(\n",
    "                                num_heads=self._nhead,\n",
    "                                key_dim=self._d_model,\n",
    "                                dropout=self._attention_dropout,\n",
    "                                )\n",
    "        self.norm = tf.keras.layers.LayerNormalization(\n",
    "                            axis=-1,\n",
    "                            dtype=tf.float32)\n",
    "        self.dropout = tf.keras.layers.Dropout(self._dropout)\n",
    "\n",
    "        super().build()\n",
    "    \n",
    "    def with_pos_embed(self, tensor, pos: Optional[tf.Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "    \n",
    "    def forward_post(self, tgt,\n",
    "                     tgt_mask: Optional[tf.Tensor] = None,\n",
    "                     query_pos: Optional[tf.Tensor] = None):\n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        tgt2 = self.self_attn(query = q, key = k, value=tgt, attention_mask=tgt_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm(tgt)\n",
    "        \n",
    "        return tgt\n",
    "    \n",
    "     def forward_pre(self, tgt,\n",
    "                    tgt_mask: Optional[tf.Tensor] = None,\n",
    "                    query_pos: Optional[tf.Tensor] = None):\n",
    "        tgt2 = self.norm(tgt)\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2 = self.self_attn(query = q, key = k, value=tgt2, attention_mask=tgt_mask)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        \n",
    "        return tgt\n",
    "    \n",
    "    def call(self, tgt,\n",
    "                tgt_mask: Optional[tf.Tensor] = None,\n",
    "                query_pos: Optional[tf.Tensor] = None):\n",
    "        if self._normalize_before:\n",
    "            return self.forward_pre(tgt, tgt_mask, query_pos)\n",
    "        return self.forward_post(tgt, tgt_mask, query_pos)\n",
    "    \n",
    "class CrossAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 dmodel,\n",
    "                 nhead,\n",
    "                 dropout=0.0,\n",
    "                 activation=\"relu\",\n",
    "                 normalize_before=False\n",
    "                ):\n",
    "        self._dmodel=dmodel\n",
    "        self._nhead=nhead\n",
    "        self._dropout=dropout\n",
    "        self._activation=activation\n",
    "        self._normalize_before=normalize_before\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self):\n",
    "        self.multihead_attn = tf.keras.layers.MultiHeadAttention(\n",
    "                                num_heads=self._nhead,\n",
    "                                key_dim=self._d_model,\n",
    "                                dropout=self._attention_dropout,\n",
    "                                )\n",
    "        self.norm = tf.keras.layers.LayerNormalization(\n",
    "                            axis=-1,\n",
    "                            dtype=tf.float32)\n",
    "        self.dropout = tf.keras.layers.Dropout(self._dropout)\n",
    "        super().build()\n",
    "        \n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, \n",
    "                     tgt, \n",
    "                     memory,\n",
    "                     memory_mask: Optional[tf.Tensor] = None,\n",
    "                     pos: Optional[tf.Tensor] = None,\n",
    "                     query_pos: Optional[tf.Tensor] = None):\n",
    "        \n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, \n",
    "                                   attention_mask=memory_mask,\n",
    "                                   )[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm(tgt)\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, \n",
    "                    tgt, \n",
    "                    memory,\n",
    "                    memory_mask: Optional[tf.Tensor] = None,\n",
    "                    pos: Optional[tf.Tensor] = None,\n",
    "                    query_pos: Optional[tf.Tensor] = None):\n",
    "        \n",
    "        tgt2 = self.norm(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, \n",
    "                                   attn_mask=memory_mask,\n",
    "                                   )[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "    def call(self, \n",
    "                tgt, \n",
    "                memory,\n",
    "                memory_mask: Optional[tf.Tensor] = None,\n",
    "                pos: Optional[tf.Tensor] = None,\n",
    "                query_pos: Optional[tf.Tensor] = None):\n",
    "        if self._normalize_before:\n",
    "            return self.forward_pre(tgt, \n",
    "                                    memory, \n",
    "                                    memory_mask,\n",
    "                                    pos, \n",
    "                                    query_pos)\n",
    "        return self.forward_post(tgt, \n",
    "                                 memory, \n",
    "                                 memory_mask,\n",
    "                                 pos, \n",
    "                                 query_pos)\n",
    "    \n",
    "class FFNLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 dim_feedforward=2048, \n",
    "                 dropout=0.0,\n",
    "                 activation=\"relu\", \n",
    "                 normalize_before=False):\n",
    "        self._dmodel=dmodel\n",
    "        self._dim_feedforward=dim_feedfoward\n",
    "        self._dropout=dropout\n",
    "        self._activation=activation\n",
    "        self._normalize_before=normalize_before\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def build(self):\n",
    "        self.linear1 = tf.keras.layers.Dense(self._dim_feedforward)\n",
    "        self.linear2 = tf.keras.layers.Dense(self._dmodel)\n",
    "        self.dropout = tf.keras.layers.Dropout(self._dropout)\n",
    "        self.norm = tf.keras.layers.LayerNormalization(\n",
    "                            axis=-1,\n",
    "                            dtype=tf.float32)\n",
    "        self.activation = tf.keras.layers.Activation(self._activation)\n",
    "        \n",
    "        super().build()\n",
    "        \n",
    "    def with_pos_embed(self, tensor, pos: Optional[tf.Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    def forward_post(self, tgt):\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, tgt):\n",
    "        tgt2 = self.norm(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def call(self, tgt):\n",
    "        if self._normalize_before:\n",
    "            return self.forward_pre(tgt)\n",
    "        return self.forward_post(tgt)\n",
    "\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 num_layers\n",
    "                 ):\n",
    "        self._input_dim=input_dim\n",
    "        self._hidden_dim=hidden_dim\n",
    "        self._output_dim=output_dim\n",
    "        self._num_layers=num_layers\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self):\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        layers = []\n",
    "        for n, k in zip([self._input_dim] + h, h + [self._output_dim]):\n",
    "            layers.append(tf.keras.layers.Dense(k))\n",
    "        self.layers = layers\n",
    "        self.activation = tf.keras.layers.Activation(\"relu\")\n",
    "        super().build()\n",
    "\n",
    "    def call(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = self.activation(layer(x)) if i < self._num_layers - 1 else layer(x)\n",
    "        return x\n",
    "    \n",
    "class MultiScaleMaskedTransformerDecoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 num_classes: int,\n",
    "                 hidden_dim: int,\n",
    "                 num_queries: int,\n",
    "                 nheads: int,\n",
    "                 dim_feedforward: int,\n",
    "                 dec_layers: int,\n",
    "                 pre_norm: bool,\n",
    "                 mask_dim: int,\n",
    "                 enforce_input_project: bool,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._in_channels=in_channels\n",
    "        self._num_classes=num_classes\n",
    "        self._hidden_dim=hidden_dim\n",
    "        self._num_queries=num_queries\n",
    "        self._nheads=nheads\n",
    "        self._dim_feedforward=dim_feedforward\n",
    "        self._dec_layers=dec_layers\n",
    "        self._pre_norm=pre_norm\n",
    "        self._mask_dim=mask_dim\n",
    "        self._enforce_input_project=enforce_input_project\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        self.transformer_self_attention_layers = []\n",
    "        self.transformer_cross_attention_layers = []\n",
    "        self.transformer_ffn_layers = []\n",
    "\n",
    "        for _ in range(self._dec_layers):\n",
    "            self.transformer_self_attention_layers.append(\n",
    "                SelfAttentionLayer(\n",
    "                    d_model=self._hidden_dim,\n",
    "                    nhead=self._nheads,\n",
    "                    dropout=0.0,\n",
    "                    normalize_before=self._pre_norm,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.transformer_cross_attention_layers.append(\n",
    "                CrossAttentionLayer(\n",
    "                    d_model=self._hidden_dim,\n",
    "                    nhead=self._nheads,\n",
    "                    dropout=0.0,\n",
    "                    normalize_before=self._pre_norm,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.transformer_ffn_layers.append(\n",
    "                FFNLayer(\n",
    "                    d_model=self._hidden_dim,\n",
    "                    dim_feedforward=self._dim_feedforward,\n",
    "                    dropout=0.0,\n",
    "                    normalize_before=self._hidden_dim,\n",
    "                )\n",
    "            )\n",
    "        self.decoder_norm = tf.keras.layers.LayerNormalization(\n",
    "                                axis=-1,\n",
    "                                dtype=tf.float32)\n",
    "        # learnable query features\n",
    "        self.query_feat = tf.keras.layers.Embedding(self._num_queries, self._hidden_dim)\n",
    "        # learnable query p.e.\n",
    "        self.query_embed = tf.keras.layers.Embedding(self._num_queries, self._hidden_dim)\n",
    "\n",
    "        # level embedding (we always use 3 scales)\n",
    "        self.num_feature_levels = 3\n",
    "        self.level_embed = tf.keras.layers.Embedding(self.num_feature_levels, self._hidden_dim)\n",
    "        self.input_proj = []\n",
    "        for _ in range(self.num_feature_levels):\n",
    "            if self._in_channels != self._hidden_dim or self._enforce_input_project:\n",
    "                self.input_proj.append(tf.keras.layers.Conv2D(filters=self.hidden_dim,\n",
    "                                      kernel_size=1,\n",
    "                                      padding='same')\n",
    "                                      )\n",
    "                \n",
    "            else:\n",
    "                self.input_proj.append()\n",
    "\n",
    "        self.class_embed = tf.keras.layers.Dense(num_classes + 1)\n",
    "        self.mask_embed = MLP(hidden_dim, hidden_dim, mask_dim, 3)\n",
    "        \n",
    "        super().build()\n",
    "        \n",
    "    def forward_prediction_heads(self, output, mask_features, attn_mask_target_size):\n",
    "        decoder_output = self.decoder_norm(output)\n",
    "        decoder_output = tf.transpose(decoder_output, perm=(0,1))\n",
    "        outputs_class = self.class_embed(decoder_output)\n",
    "        mask_embed = self.mask_embed(decoder_output)\n",
    "        outputs_mask = tf.einsum(\n",
    "            \"bqc,bhwc->bhwq\", mask_embed, mask_features)\n",
    "\n",
    "        # NOTE: prediction is of higher-resolution\n",
    "        # [B, Q, H, W] -> [B, Q, H*W] -> [B, h, Q, H*W] -> [B*h, Q, HW]\n",
    "        attn_mask = tf.image.resize(outputs_mask, attn_mask_target_size, method=tf.image.ResizeMethod.BILINEAR)\n",
    "        # If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.\n",
    "        attn_mask = tf.keras.activations.sigmoid(attn_mask)\n",
    "        attn_mask = tf.reshape(attn_mask, shape=[-1, attn_mask.shape[2]])\n",
    "        attn_mask = tf.expand_dims(attn_mask, axis=1)\n",
    "        attn_mask = tf.tile(attn_mask, multiples=[1, self._num_heads, 1, 1])\n",
    "        attn_mask = tf.reshape(attn_mask, shape=[-1, attn_mask.shape[1], attn_mask.shape[2]])\n",
    "        attn_mask = tf.cast(attn_mask < threshold, dtype=tf.bool)\n",
    "\n",
    "        return outputs_class, outputs_mask, attn_mask\n",
    "\n",
    "    def call(self, x, mask_features, mask = None):\n",
    "        \n",
    "        assert len(x) == self.num_feature_levels\n",
    "        src = []\n",
    "        pos = []\n",
    "        size_list = []\n",
    "\n",
    "        # disable mask, it does not affect performance\n",
    "        del mask\n",
    "        \n",
    "        for i in range(self.num_feature_levels):\n",
    "            size_list.append(x[i].shape[-2:])\n",
    "            \n",
    "            pos_embed = position_embedding_sine(\n",
    "                x[i], num_pos_features=self._hidden_dim)\n",
    "            pos_embed = tf.reshape(pos_embed, [batch_size, -1, self._hidden_size])\n",
    "            pos_embed = tf.reshape(tf.transpose(pos_embed, perm=[0, 2, 1]), shape=(-1,))\n",
    "            \n",
    "            pos.append(pos_embed)\n",
    "            src.append(tf.reshape(tf.transpose(self.input_proj[i](x[i], perm=[0, 2, 1]), shape=(-1,)) + self.level_embed.weight[i][None, :, None])\n",
    "\n",
    "            # flatten NxCxHxW to HWxNxC\n",
    "            pos[-1] = tf.keras.layers.Permute((2,0,1))(pos[-1])\n",
    "            src[-1] = tf.keras.layers.Permute((2,0,1))(src[-1])\n",
    "\n",
    "\n",
    "        _, bs, _ = src[0].shape\n",
    "        # QxNxC\n",
    "        query_embed = tf.keras.layers.tile(tf.expand_dims(self.query_embed.weight, 1), (1, bs, 1))\n",
    "        output = tf.keras.layers.tile(tf.expand_dims(self.query_feat.weight, 1), (1, bs, 1))\n",
    "\n",
    "        predictions_class = []\n",
    "        predictions_mask = []\n",
    "\n",
    "        # prediction heads on learnable query features\n",
    "        outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[0])\n",
    "        predictions_class.append(outputs_class)\n",
    "        predictions_mask.append(outputs_mask)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            level_index = i % self.num_feature_levels\n",
    "            attn_mask[torch.where(attn_mask.sum(-1) == attn_mask.shape[-1])] = False\n",
    "            # attention: cross-attention first\n",
    "            output = self.transformer_cross_attention_layers[i](\n",
    "                output, \n",
    "                src[level_index],\n",
    "                memory_mask=attn_mask,\n",
    "                pos=pos[level_index], \n",
    "                query_pos=query_embed\n",
    "            )\n",
    "\n",
    "            output = self.transformer_self_attention_layers[i](\n",
    "                output, tgt_mask=None,\n",
    "                query_pos=query_embed\n",
    "            )\n",
    "            \n",
    "            # FFN\n",
    "            output = self.transformer_ffn_layers[i](\n",
    "                output\n",
    "            )\n",
    "\n",
    "            outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + 1) % self.num_feature_levels])\n",
    "            predictions_class.append(outputs_class)\n",
    "            predictions_mask.append(outputs_mask)\n",
    "\n",
    "        assert len(predictions_class) == self.num_layers + 1\n",
    "\n",
    "        out = {\n",
    "            'pred_logits': predictions_class[-1],\n",
    "            'pred_masks': predictions_mask[-1],\n",
    "        }\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
